{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e3f09c-e12e-43f7-9b25-eb884d2b9675",
   "metadata": {},
   "source": [
    "# Talking to Ourselves - Simulated Conversation with Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb42e47-290b-4dd8-8139-4eddfa57181d",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccdebf-154a-4da4-9eac-c585c1026eb0",
   "metadata": {},
   "source": [
    "## Chatbots\n",
    "Chatbots hold a special place in the history and philosphy of artificial intelligence because of the Turing test. Originally proposed by Alan Turing in 1950 as the *Imitation Game*, the Turing test considers the question, 'Can Machines think?' by asking the question, 'Can a human distinguish between a machine and another human?' (cite). While many researchers have explored and critcized Turing's idea, creating chatbots that are able to simulate human-like responses remains a tenant of artificial intelligence research.\n",
    "\n",
    "Today, chatbots can most often be found as tools to aid in customer support or psychotherapy, or as agents of fun or entertainment. To be effective, a chatbot must be able to do the following things:\n",
    "- Parse user input\n",
    "- intepret the meaning of the user input\n",
    "- provide an appropriate response\n",
    "\n",
    "Chatbots can be categorized in many different ways like which response architecture is used to generate responses, conversational domains, or by who is able to initiative conversation. Response architecture can be rule-based, retrieval-based, or generative. In rule-based models, the system generated responses are entirely predefined and are returned according to a series of rules, typically using a decision tree and clearly defined possible outputs for each step in the conversation. In retrieval-based models, machine learning models are typically used to intepret user queries and responses are pulled from an existing corpus of dialogs. Like rule-based models, retreival-based models typically rely on predefined responses, but can improve their selection of responses over time. Generative models form original responses based on user input. This project will use a generative model to simulate more human-like responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d0a94-6380-4892-8a90-97d32adb7131",
   "metadata": {},
   "source": [
    "## Something about our data, transformers, model... something?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6557a-99e3-4a5b-b7a6-c83563e5df33",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "\n",
    "### Packages\n",
    "We will be using the following packages:\n",
    "- re                 : for regular expression matching operations.\n",
    "- time               : for something... (**remember to explain this**)\n",
    "- NumPy              : for use with array objects\n",
    "- TensorFlow         : for building our transformer\n",
    "- TensorFlow_datasets: for preparing and processing our dataset\n",
    "\n",
    "re and time are part of the Python Standard Library and usually come pre-installed with Python. NumPy can be installed using either conda or pip. Installing TensorFlow, and TensorFlow Datasets can be a lengthy and challenging process so I recommend following the instructions on the TensorFlow website. \n",
    "\n",
    "For this project, we are using the Paperspace Gradient platform because of its quick setup, ease of use, and low operating costs. Paperspace has several template runtimes that come with NumPy and TensorFlow pre-installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb6c1d5-c4e4-474b-bd7a-660d2c6d8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependencies that aren't already installed\n",
    "\n",
    "#!pip install tensorflow_datasets -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a144b3-e7eb-4806-be92-423ed52aca94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.9.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tf.keras.utils.set_random_seed(1234)\n",
    "\n",
    "#GPU Initialization\n",
    "strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(f\"Tensorflow version {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f7d4fe-ccb3-40b9-90c5-ff0e816a902a",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Hyperparameter tuning is beyond the scope of this project. As such, we will be using the following arbitrarily determined settings to attempt to balance time and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30373f3-d445-4cb3-84f1-233324f0a5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Maximum sentence length\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "#Maximum number of samples to preprocess\n",
    "MAX_SAMPLES = 50000\n",
    "\n",
    "#For tf.data.Dataset\n",
    "BATCH_SIZE = 256 * strategy.num_replicas_in_sync\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "#For Transformer\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3adc33c-0efb-4ff7-a91c-3d7e0dbc1b79",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Generating dialog is typically achieved by feeding machine translation models a dataset that is separated by speakers and typically described as a question and an answer. For this project, we will be using the Cornell Movie-Dialogs Corpus as our dataset, which contains 220,579 conversational exchanges between 10,292 pairs of movie characters (cite). This dataset is sufficiently large, rich, and processed. This will allow us to focus on the intent of this project - learning how to build a language model using transformers.\n",
    "\n",
    "The dataset is broken into two documents:\n",
    "- movie_conversations.txt: Contains a list of conversation IDs, each containing a list of line IDs for that conversation.\n",
    "- movie_lines.text       : Contains the text associated with each conversation ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def63067-f246-4f8d-ad2a-cb4d1a26bf3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Download the dataset\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    \"cornell_movie_dialogs.zip\",\n",
    "    origin=\"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "#Set the path to the dataset\n",
    "path_to_dataset = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\"\n",
    ")\n",
    "#Set the path to the list of conversations\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset, \"movie_conversations.txt\")\n",
    "\n",
    "#Set the path to the text associated with each conversation\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, \"movie_lines.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b8080-def2-4b15-a7f2-03671a0f0ef0",
   "metadata": {},
   "source": [
    "### Preparing the Dataset\n",
    "\n",
    "To train our model, our data needs to be structured in a way that our model can use it. To do that, we will be doing the following:\n",
    "1. Build a dictionary from the movie_lines.txt file with the movie lines as the keys and the text of the lines as the values\n",
    "2. Preprocessing each sentence by converting to lowercase and removing white space, contractions, and special characters.\n",
    "3. Iterate through each conversation in the movie_conversations.txt file.\n",
    "4. Iterate through each line in a conversations, adding that line to the list of questions (input sentences) and the following line to the list of answers (output sentences).\n",
    "5. Build a tokenizer using TensorFlow Datasets SubwordTextEncoder.\n",
    "6. Tokenize each sentence.\n",
    "7. Add START_TOKENS and END_TOKENS to indicate the start and end of each sentence.\n",
    "8. Filter out sentences that have more than 60 tokens. \n",
    "9. Pad tokenized sentences that have less than 60 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6583239f-f0ca-417e-b974-de787b5c084c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    #convert to all lower case and remove white space at the beginning and end\n",
    "    sentence = sentence.lower().strip()\n",
    "    #creating a space between a word and the punctuation following it\n",
    "    # ex. \"He is a boy.\" -> \"he is a boy .\"\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    #removing contractions\n",
    "    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
    "    sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
    "    sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
    "    sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
    "    sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
    "    sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
    "    sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
    "    sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
    "    sentence = re.sub(r\"n't\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
    "    sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
    "    #replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    #remove white space at the beginning and end\n",
    "    sentence = sentence.strip() \n",
    "    return sentence\n",
    "\n",
    "def load_conversations():\n",
    "    #dictionary of line id to text\n",
    "    id2line = {}\n",
    "    with open(path_to_movie_lines, errors=\"ignore\") as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        #Remove the new line \"\\n\"\n",
    "        #\" +++$+++ \" is being used as a field separator in all of the files\n",
    "        #Use this to separate text into lists, Should look something like this:\n",
    "        # Conversation Line ID, Character (Speaker) ID, Movie ID, Character Name, Text#        \n",
    "        parts = line.replace(\"\\n\", \"\").split(\" +++$+++ \")\n",
    "        #Set the Conversation line ID as a key and the text as the value\n",
    "        id2line[parts[0]] = parts[4]\n",
    "        \n",
    "    inputs, outputs = [], []\n",
    "    with open(path_to_movie_conversations, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace(\"\\n\", \"\").split(\" +++$+++ \")\n",
    "        #Set each conversation as a list of line IDs (parts[3])\n",
    "        conversation = [line[1:-1] for line in parts[3][1:-1].split(\", \")]\n",
    "        #For each conversation, set input as line 1, output as line 2, input as line 2, output as line 3... etc.\n",
    "        #Process each sentence by removing special characters, etc. (see preprocess_sentence above)\n",
    "        for i in range(len(conversation) - 1):\n",
    "            inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "            outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "            if len(inputs) >= MAX_SAMPLES:\n",
    "                return inputs, outputs\n",
    "    return inputs, outputs\n",
    "\n",
    "#Define conversational starters or inputs as questions (or prompts)\n",
    "#Define conversational reactions or outputs as answers (or replies)\n",
    "questions, answers = load_conversations()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29a75d-26d3-4d94-bdcc-30c287ab0688",
   "metadata": {},
   "source": [
    "Let's take a look at some sample questions and answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad30b342-f810-4193-835b-00d54a8f5446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question: what would he say ?\n",
      "Sample Answer: who cares ?\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample Question: {questions[250]}\")\n",
    "print(f\"Sample Answer: {answers[250]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945420d1-7776-4cb9-9870-bff832eebf1c",
   "metadata": {},
   "source": [
    "TensorFlow Dataset's SubWordTextEncoder builds a tokenizer from our dataset by mapping text to an ID and an ID to a text. This method has known bug and performance issues and is not being maintained (cite https://github.com/tensorflow/datasets/issues/2879); however, it currently meets our needs. Future work will require alternative methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c86204-4c8b-428e-b063-4365fdb2ca21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Build tokenizer using tfds for both questions and answers\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size = 2**13)\n",
    "\n",
    "#Define start and end token to indicate the start and end of a sentence\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "#Vocabulary size plus start and end token\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd1fcf9d-dbbc-4ddb-8c8b-a2d302e85eab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database is locked')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "#Tokenize, filter and pad sentences\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        #tokenize sentence\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "        #check tokenized sentence max length\n",
    "        #Filter sentence length > 60\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "            \n",
    "            \n",
    "    #pad tokenized sentences\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding=\"post\"\n",
    "    )\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding=\"post\"\n",
    "    )\n",
    "    \n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c5efa3-61f3-44e9-9107-3931122484dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8226\n",
      "Number of samples: 211082\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Number of samples: {len(questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f89f15-1b99-4330-9f48-6aab022cdceb",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "### Transformers, an Overview\n",
    "\n",
    "- A transformer takes an input, passes it through an encoder component, a decoder component, and generates an output. In Natural Language Processing, the input is a sentence such as, \"what would he say ?\" Each word in this sentence would first be converted into a word embedding.\n",
    "- The encoder component is a stack of encoder layers that all receive a list of vectors as input. The first encoder layers receives the word embedding of the input, and the subsequent encoder layers would receive the output of the previous encoder layer. The encoder outputs a representation of the input sentence.\n",
    "- The decoder takes the transformer's previous outputs and the encoder's outputs as inputs. The decoder aggregates its inputs and the output its generated so far to generate one word at a time until it reaches an END_TOKEN.\n",
    "\n",
    "\n",
    "### Setting Up Our Input Pipeline\n",
    "The tf.data.Dataset API enables us to expedite the training process by using features like caching and prefetching. This project will sacrifice function and accuracy of the model for processing time and efficiency by using *teacher-forcing*. Rather than use the models predictions, Teacher-Forcing feeds the true output to the next step at the current timestep. In the code below, the dictionary keys \"dec_inputs\" and \"outputs\" are for teacher-forcing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a54fc0-fd4d-4841-82c7-f27181454b02",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.630407Z",
     "iopub.status.idle": "2022-11-21T19:24:56.630792Z"
    }
   },
   "outputs": [],
   "source": [
    "#decoder inputs use the previous target as input\n",
    "#remove START_TOKEN from targets\n",
    "#Uses the tensorflow API to construct a Dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\"inputs\": questions, \"dec_inputs\": answers[:,:-1]},\n",
    "        {\"outputs\": answers[:, 1:]},\n",
    "    )\n",
    ")\n",
    "#Cache the dataset, saving time operations (like file opening and data reading from being executed during each epoch\n",
    "dataset = dataset.cache()\n",
    "#.shuffle maintains an internal buffer of elements, reducing memory footprint\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "#set the batch size\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "#Prefetching overlaps the preprocessing and model execution of a training step\n",
    "#While the model is executing training step s, the input pipeline is reading the data for step s+1, reducing step time\n",
    "#tf.data.AUTOTUNE prompts the tf.data runtime to dynamically adjust the number of elements to prefetch at runtime\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39f3ed-702d-4249-a1c5-f081f77c6d5b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.634228Z",
     "iopub.status.idle": "2022-11-21T19:24:56.634773Z"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534106d6-b918-420f-b994-360a9917a82a",
   "metadata": {},
   "source": [
    "### Model, more specific\n",
    "A positional encoding layer is added to the input embeddings before the encoder layers.\n",
    "Any number of encoder layers. Each encoder layer consists of two sub layers.\n",
    "1. First, a multi-headed attention layer.\n",
    "2. A feed-forward neural network layer.\n",
    "A positional encoding layer is added to the output embeddings before the decoder layers.\n",
    "Any number of decoder layers. Each decoder layer consists of 3 sub layers.\n",
    "- Each decoder layer consists of three sub-layers.\n",
    "1. First, a multi-headed attention layer.\n",
    "2. An encoder-decoder attention layer.\n",
    "3. A feed-forward neural network.\n",
    "Each sublayer has a residual connection around it and after each sub-layer, a layer normalization step occurs.\n",
    "A Linear Layer takes the output of the decoder stacks and projects the vector produced by the stack of decoders into a logits vector.\n",
    "A softmax layer converts the values in the logits vector into probabilities.\n",
    "The cell in the logits vector with the highest probability is chosen, and the word associated with it is produced as our output.\n",
    "\n",
    "### Attention\n",
    "Self-attention is the mechanism by which a transformer is able to add context of other relevant words into the one we're currently processing. We will use the following sentence as an example: \n",
    "\n",
    "> \"The dog fetched the ball but he didn't bring it back.\"\n",
    "\n",
    "The word \"it\" is a pronoun used to refer to something previously mentioned, and self-attention enables our model to associate the word \"it\" with \"ball\" rather than with another word in the sentence like dog. **Multi-Headed Attention** splits self-attention into multiple heads, then concatenates the output of each head and multiplies them by an additional weights matrix before sending the information to the feed-forward neural network layer.\n",
    "Self-Attention can be done by with the following steps:\n",
    "1. Multiply each of input vectors by 3 matrices created during the training process to create 3 vectors: Query, Key, and Value.\n",
    "2. Obtain the dot product of the query vectory and the key vector of the respective word we're scoring.\n",
    "- For example, if we're scoring the word in position 1, score_1 is the dot product of query_1 and key_1, score_2 is the dot product of query_1 and key_2.\n",
    "3. Scale the dot product obtained in step 2 by a factor of the square root of the depth.\n",
    "- This allows for more stable gradients.\n",
    "4. Pass the scaled dot product through a softmax operation.\n",
    "- Softmax makes all of the scores positive and add up to 1.\n",
    "5. Multiply the softmax score by each value vector.\n",
    "6. Sum the weighted value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee30cf-ad64-472e-8e1f-1268b3fa6309",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.635768Z",
     "iopub.status.idle": "2022-11-21T19:24:56.636129Z"
    }
   },
   "outputs": [],
   "source": [
    "#Module that computes the attention weights for the input and\n",
    "#produces an output vector with encoded information on how each word should\n",
    "#attend to all other words in the sequence\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights.\"\"\"\n",
    "    #Matrix mutliply query by transposed key to generate to produce a score matrix\n",
    "    #Score matrix determines how much focus should a word be put on other words\n",
    "    #Each word will have a score that corresponds to other words in a timestep\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "    \n",
    "    #scale matmul_qk\n",
    "    #Scales the scores down to allow for more stable gradients\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "    \n",
    "\n",
    "    #add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += mask * -1e9\n",
    "        \n",
    "    #softmax gives probability values between 0 and 1 for attention weights\n",
    "    #softmax heightens higher scores and depresses lower scores\n",
    "    #This allows the model to be more confident on which words to attend to\n",
    "    #softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "    \n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca912c-2954-4e28-81bd-b2a4526a30fd",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention\n",
    "Instead of one single attention head, query, key, and value vectors are split into multiple heads. This allows the model to jointly attend to information at different positions from different representational spaces. After the split, each head has a reduced dimensionality, leading to a total computation cost that is the same as a single head attention with full dimensionality. The output of each head is concatenated into one matrix and multiplied by an additional weights matrix which was trained with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e56f1-6551-4927-9eb1-c75c028305eb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.637150Z",
     "iopub.status.idle": "2022-11-21T19:24:56.637501Z"
    }
   },
   "outputs": [],
   "source": [
    "#Setup a multi-head attention layer\n",
    "#layers are functions with a known mathematical structure that can be reused\n",
    "#and have trainable variables\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        #check to see if the data model is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0\n",
    "        #Inherit the methods of tf.keras.layers.Layer\n",
    "        super(MultiHeadAttentionLayer, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        #setup linear dense layers for the query, key, and value inputs\n",
    "        #Each \"neuron\" in the dense layer receives input from all neurons of its previous layer\n",
    "        #Dense layer performs a matrix-vector multiplication, values are paremeters that can be trained\n",
    "        #and updated with the help of backpropogation\n",
    "        #units represents the output size of the layer (in this case, the size of the data model)\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super(MultiHeadAttentionLayer, self).get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"d_model\": self.d_model,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "    #Split query, key, and value into multiple heads\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.keras.layers.Lambda(\n",
    "            lambda inputs: tf.reshape(\n",
    "                inputs, shape=(batch_size, -1, self.num_heads, self.depth)\n",
    "            )\n",
    "        )(inputs)\n",
    "        return tf.keras.layers.Lambda(\n",
    "            lambda inputs: tf.transpose(inputs, perm=[0, 2, 1, 3]) #perm sets the dimension of the returned matrix\n",
    "        )(inputs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = (\n",
    "            inputs[\"query\"],\n",
    "            inputs[\"key\"],\n",
    "            inputs[\"value\"],\n",
    "            inputs[\"mask\"],\n",
    "        )\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        #Linear Layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "    \n",
    "        #split heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "    \n",
    "        #scaled dot-product attention\n",
    "        #applied to each head (broadcast for efficiency)\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        scaled_attention = tf.keras.layers.Lambda(\n",
    "            lambda scaled_attention: tf.transpose(scaled_attention, perm=[0,2,1,3])\n",
    "        )(scaled_attention)\n",
    "    \n",
    "        #concatenation of heads\n",
    "        concat_attention = tf.keras.layers.Lambda(\n",
    "            lambda scaled_attention: tf.reshape(\n",
    "                scaled_attention, (batch_size, -1, self.d_model)\n",
    "            )\n",
    "        )(scaled_attention)\n",
    "    \n",
    "        #final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f2148e-b67f-453c-b02f-692e4b78bdc3",
   "metadata": {},
   "source": [
    "### Masking\n",
    "For our model to work, the embedding of every sentence needs to be the same size. To achieve this, we added padding tokens to any sentence that is not as long as our longest sentence. Additionally, our decoder layer is autoregressive, so we need to be able to prevent it from conditioning to future tokens. To accomplish both of these things, we need helper-functions to mask the pad tokens and future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf24a4d-4bd2-4006-aad9-97e98308322d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.638122Z",
     "iopub.status.idle": "2022-11-21T19:24:56.638491Z"
    }
   },
   "outputs": [],
   "source": [
    "#Mask all the pad tokens (value 0) in the batch to ensure the model does not treat padding as input\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb92d293-9bfa-4606-aa69-bf0ae37f48f5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.640424Z",
     "iopub.status.idle": "2022-11-21T19:24:56.640734Z"
    }
   },
   "outputs": [],
   "source": [
    "#Decoder layer is autoregressive and generates sequence word by word\n",
    "#Prevent the decoder from conditioning to future tokens\n",
    "#Add a look-ahead mask, zeroing out future tokens\n",
    "#ex. to predict a 4th word, only the first, second, and 3rd word will be used\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30a93e-563e-46b3-8b77-decb103a0e75",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "Neither Transformers nor embeddings contain any information about the relative position of a word in a sentence, so we will be adding a positional encoding vector to the embedding vector. By adding positional encoding to the embedding vector, words become closer to each other based on their position in a sentence and in their similarity of meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec71df-8d81-4fb7-9d05-95e7ffc5dd78",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.642439Z",
     "iopub.status.idle": "2022-11-21T19:24:56.642758Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.position = position\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"position\": self.position,\n",
    "                \"d_model\": self.d_model,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "    \n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1/ tf.pow(10000, (2 * (i //2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position = tf.range(position, dtype = tf.float32)[:, tf.newaxis],\n",
    "            i = tf.range(d_model, dtype = tf.float32)[tf.newaxis, :],\n",
    "            d_model = d_model,\n",
    "        )\n",
    "        #apply sin to even index in the array\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        #apply cos to odd index in the array\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        pos_encoding = tf.concat([sines, cosines], axis = -1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, : tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e5203a-d7ae-4c67-9550-b12865d9577d",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "Each of our encoder layers uses 2 sublayers:\n",
    "1. Multi-Head Attention\n",
    "2. Feed-Forward neural network.\n",
    "- Our feed-forward neural network is composed of 2 Dense layers and 1 dropout layer.\n",
    "Each sublayer has a residual connection around it followed by a layer normalization. \n",
    "\n",
    "#### A little explanation\n",
    "Each dense layer applies an activation function to the dot product of the input and weight data, plus any bias.\n",
    "Residual connections enable some data to skip some layers in a neural network, helping to prevent the vanishing gradient problem in deep networks.\n",
    "Dropout randomly sets input units to 0 with a predefined frequency of some rate (we set 0.1) at each step during the training time. This helps prevent over-fitting. Inputs not set to 0 are scaled up by 1/(1-rate). This means the sum of all the inputs remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc2b65-0301-4432-9275-f4a566fa4e22",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.644412Z",
     "iopub.status.idle": "2022-11-21T19:24:56.644728Z"
    }
   },
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name = \"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1,1,None), name = \"padding_mask\")\n",
    "    \n",
    "    attention = MultiHeadAttentionLayer(d_model, num_heads, name=\"attention\")(\n",
    "        {\"query\": inputs, \"key\": inputs, \"value\": inputs, \"mask\": padding_mask}\n",
    "    )\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    add_attention = tf.keras.layers.add([inputs, attention])\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(units=units, activation = \"relu\")(attention)\n",
    "    outputs = tf.keras.layers.Dense(units = d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    add_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs = outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eeb401-d2f4-4292-b67a-e3964a03aca5",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The encoder has 3 parts:\n",
    "1. Input Embedding\n",
    "2. Positional Encoding\n",
    "3. Any number of encoder layers (we're using 6)\n",
    "\n",
    "Our input is put through an embedding. The positional encoding is added to the embedding. The sum of these two embeddings is the input to the encoder layers. The output of the encoder layer is input for the decoder layer's encoder-decoder attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07bb87a-13c5-4dc4-867e-6170a0475c5d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.646521Z",
     "iopub.status.idle": "2022-11-21T19:24:56.646843Z"
    }
   },
   "outputs": [],
   "source": [
    "def encoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1,1,None), name = \"padding_mask\")\n",
    "    \n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.keras.layers.Lambda(\n",
    "        lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    )(d_model)\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "        \n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs = outputs, name=name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec2a88-f629-4b22-9e70-0a21d29cbb3a",
   "metadata": {},
   "source": [
    "## Decoder Layer\n",
    "Each of the decoder layers consists of 3 sub-layers:\n",
    "1. Masked Multi-Headed Attention (with look ahead mask and padding mask)\n",
    "2. An Encoder-Decoder Attention Layer (Multi-Head Attention with padding mask)\n",
    "3. 2 Dense layers followed by dropout\n",
    "Each sub-layer has a residual connection around it followed by layer normalization.\n",
    "\n",
    "### A little explanation\n",
    "The first masked multi-headed attention layer in the first decoder layer takes previous outputs as its inputs.\n",
    "The encoder-decoder attention layer takes they key and value outputs from the encoder output and the query from the previous layer. This enables the decoder to predict the next word by looking at its own output and self-attending to its own input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5b137-5925-4dc5-af6e-33d59753220c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.648567Z",
     "iopub.status.idle": "2022-11-21T19:24:56.648886Z"
    }
   },
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name = \"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name = \"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name = \"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = \"padding_mask\")\n",
    "    \n",
    "    attention1 = MultiHeadAttentionLayer(d_model, num_heads, name=\"attention_1\")(\n",
    "        inputs={\n",
    "            \"query\": inputs,\n",
    "            \"key\": inputs,\n",
    "            \"value\": inputs,\n",
    "            \"mask\": look_ahead_mask,\n",
    "        }\n",
    "    )\n",
    "    add_attention = tf.keras.layers.add([attention1, inputs])\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "    \n",
    "    attention2 = MultiHeadAttentionLayer(d_model, num_heads, name=\"attention_2\")(\n",
    "        inputs = {\n",
    "            \"query\": attention1,\n",
    "            \"key\": enc_outputs,\n",
    "            \"value\": enc_outputs,\n",
    "            \"mask\": padding_mask,\n",
    "        }\n",
    "    )\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    add_attention = tf.keras.layers.add([attention2, attention1])\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(units=units, activation=\"relu\")(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    add_attention = tf.keras.layers.add([outputs, attention2])\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa40fc-1371-4ba4-b596-8253e53c9537",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "The decoder has 3 parts:\n",
    "1. Input Embedding\n",
    "2. Positional Encoding\n",
    "3. Any number of encoder layers (we're using 6)\n",
    "\n",
    "The target is put through an embedding. The positional encoding is added to the embedding. The sum of these two embeddings is the input to the decoder layers. The output of the decoder is the input of the final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de550d07-1285-4676-acad-51eaba0e8fa3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.650909Z",
     "iopub.status.idle": "2022-11-21T19:24:56.651257Z"
    }
   },
   "outputs": [],
   "source": [
    "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"decoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.keras.layers.Lambda(\n",
    "        lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    )(d_model)\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"decoder_layer_{}\".format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358c309-04a6-402e-8ae8-a7ab7934efe9",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Transformer consists of the encoder, decoder, and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d0ca8-8060-493c-ace9-a21c3f909bf1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.653023Z",
     "iopub.status.idle": "2022-11-21T19:24:56.653355Z"
    }
   },
   "outputs": [],
   "source": [
    "def transformer(\n",
    "    vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"transformer\"\n",
    "):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None), name=\"enc_padding_mask\"\n",
    "    )(inputs)\n",
    "    # mask the future tokens for decoder inputs at the 1st attention block\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask, output_shape=(1, None, None), name=\"look_ahead_mask\"\n",
    "    )(dec_inputs)\n",
    "    # mask the encoder outputs for the 2nd attention block\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None), name=\"dec_padding_mask\"\n",
    "    )(inputs)\n",
    "\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044300c-0894-4880-b15c-19a67f9fc9ec",
   "metadata": {},
   "source": [
    "## Train model\n",
    "### Loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02699b7-2d4c-4e0c-860a-17b22f6f7346",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.655193Z",
     "iopub.status.idle": "2022-11-21T19:24:56.655636Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=\"none\"\n",
    "    )(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591d9729-7603-4df6-9fcc-8bf0d06a217c",
   "metadata": {},
   "source": [
    "### Custom learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d933e86-54a3-42c8-bba0-5316bcaccb80",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.656317Z",
     "iopub.status.idle": "2022-11-21T19:24:56.656656Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.constant(d_model, dtype=tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"d_model\": self.d_model, \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.multiply(\n",
    "            tf.math.rsqrt(self.d_model), tf.math.minimum(arg1, arg2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768c2f6-c998-447c-9c55-d9df0be54a81",
   "metadata": {},
   "source": [
    "Initialize and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3cbea-6568-48bc-89b3-987388e1ba61",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.657268Z",
     "iopub.status.idle": "2022-11-21T19:24:56.657583Z"
    }
   },
   "outputs": [],
   "source": [
    "# clear backend\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "# initialize and compile model within strategy scope\n",
    "with strategy.scope():\n",
    "    model = transformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        units=UNITS,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c0ca8-a08a-445b-8d40-9f7a2ee1dd26",
   "metadata": {},
   "source": [
    "Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e547be-1b9d-407f-b101-2c26920b610b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b41c753-3a0a-40ef-a4e8-e9b2619f01b4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.658254Z",
     "iopub.status.idle": "2022-11-21T19:24:56.658601Z"
    }
   },
   "outputs": [],
   "source": [
    "#If your have already built the model, then comement out this box.\n",
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce4ecd6-b11e-4cc8-bf78-073402ff0438",
   "metadata": {},
   "source": [
    "Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74207a93-5226-4859-b2b8-128ae05620ca",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.659350Z",
     "iopub.status.idle": "2022-11-21T19:24:56.659674Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30913add-2fbf-4b2d-a37a-d38cac53ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If your have already built the model, then comement out this box.\n",
    "tf.keras.models.save_model(model, filepath=filename, include_optimizer=False)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf96f0-c025-452f-8183-4ee0f9e9f10f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.660280Z",
     "iopub.status.idle": "2022-11-21T19:24:56.660606Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7911159-c19c-420b-a171-3bc53b782bfe",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T19:24:56.661217Z",
     "iopub.status.idle": "2022-11-21T19:24:56.661529Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    filename,\n",
    "    custom_objects={\n",
    "        \"PositionalEncoding\": PositionalEncoding,\n",
    "        \"MultiHeadAttentionLayer\": MultiHeadAttentionLayer,\n",
    "    },\n",
    "    compile=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa46eb-ef09-4881-b97e-8c663bc4d3c0",
   "metadata": {},
   "source": [
    "Adding our model into a chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70000bdd-36cc-44f5-b132-72df2afb58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    exit_commands = (\"goodbye\", \"quit\")\n",
    "    \n",
    "    def start_chat(self):\n",
    "        user_response = input(\"Hi, I'm a chatbot trained on dialog from movies. Would you like to chat with me? \\n\")\n",
    "        \n",
    "        #If the user input contains goodbye or quit, exit the chatbot\n",
    "        if user_response in self.exit_commands:\n",
    "            print(\"Ok, have a great day!\")\n",
    "            return\n",
    "        \n",
    "        self.chat(user_response)\n",
    "        \n",
    "    def chat(self, reply):\n",
    "        while not self.make_exit(reply):\n",
    "            reply = input(self.generate_response(reply))\n",
    "            \n",
    "        \n",
    "    def inference(self, sentence):\n",
    "        sentence = preprocess_sentence(sentence)\n",
    "        \n",
    "        sentence = tf.expand_dims(\n",
    "            START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0\n",
    "        )\n",
    "        \n",
    "        output = tf.expand_dims(START_TOKEN, 0)\n",
    "        \n",
    "        for i in range(MAX_LENGTH):\n",
    "            predictions = model(inputs=[sentence, output], training=False)\n",
    "            \n",
    "            #select the last word from the seq_len dimension\n",
    "            predictions = predictions[:, -1:, :]\n",
    "            predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "            \n",
    "            #return the result if the predicted_id is equal to the end token\n",
    "            if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "                break\n",
    "                \n",
    "            #concatenated the predicted_id to the output which is given to the decoder as its input\n",
    "            output = tf.concat([output, predicted_id], axis=-1)\n",
    "            \n",
    "        return tf.squeeze(output, axis=0)\n",
    "    \n",
    "    def generate_response(self, user_input):\n",
    "        prediction = self.inference(user_input)\n",
    "        response = tokenizer.decode(\n",
    "            [i for i in prediction if i < tokenizer.vocab_size]\n",
    "        )\n",
    "        return \"Chatty McChatbot: \" + response + \"\\n User Input:\"     \n",
    "            \n",
    "    def make_exit(self, reply):\n",
    "        #If the user input contains goodbye or quit, exit the chatbot\n",
    "        for exit_command in self.exit_commands:\n",
    "            if exit_command in reply:\n",
    "                print(\"Okay, have a great day!\")\n",
    "                return True\n",
    "            \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95f3749c-c5d8-4034-88d3-a32d06ca7167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hi, I'm a chatbot trained on dialog from movies. Would you like to chat with me? \n",
      " That sounds like fun\n",
      "i will be right back .\n",
      " User Input: What?\n",
      "i do not know . i am just guessing .\n",
      " User Input: You just said you wanted to chat\n",
      "i think you are cute .\n",
      " User Input: exit\n",
      "that is wrong ?\n",
      " User Input: quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, have a great day!\n"
     ]
    }
   ],
   "source": [
    "chatty_mcchatface = ChatBot()\n",
    "chatty_mcchatface.start_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
